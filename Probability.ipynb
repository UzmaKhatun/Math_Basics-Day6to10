{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a7ddfe4-6fa7-4d52-828a-87b7fd865aaa",
   "metadata": {},
   "source": [
    "# Day-9 Of \n",
    "# <b>#100 Days of Machine Learning</b>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc8a1ab-dfd6-47e3-bfcd-08fb7bdf5029",
   "metadata": {},
   "source": [
    "# ðŸ”¹Probability : Basics, Distributions, and Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6415f996-f64f-4911-b23f-8dd9b2d07925",
   "metadata": {},
   "source": [
    "# ðŸ”¸I. Probability Basics\n",
    "## Definition\n",
    "### Probability is the measure of the likelihood that an event will occur. It is quantified as a number between 0 and 1, where 0 indicates impossibility and 1 indicates certainty.\n",
    "### Experiment: A process that results in well-defined outcomes.\n",
    "### Sample Space (S): The set of all possible outcomes of an experiment.\n",
    "### Event (E): A subset of the sample space, i.e., a collection of one or more outcomes.\n",
    "### Probability of an Event (P(E)): The number of favorable outcomes divided by the total number of possible outcomes, assuming all outcomes are equally likely.\n",
    "### $[ P(E) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}} = \\frac{|E|}{|S|} ]$\n",
    "\n",
    "## Key Concepts\n",
    "### Mutually Exclusive Events : Two events A and B are mutually exclusive if they cannot occur at the same time. $( P(A \\cap B) = 0 )$.\n",
    "### Independent Events : Two events A and B are independent if the occurrence of one does not affect the probability of the other. $( P(A \\cap B) = P(A) \\times P(B) ).$\n",
    "### Conditional Probability $(P(A|B))$ : The probability of event A occurring given that event B has already occurred.\n",
    "$[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} ]$\n",
    "\n",
    "## Examples\n",
    "### Rolling a Fair Six-Sided Die:\n",
    "#### Sample Space: $( S = {1, 2, 3, 4, 5, 6} ), ( |S| = 6 )$\n",
    "#### Event E: Rolling an even number, $( E = {2, 4, 6} ), ( |E| = 3 )$\n",
    "#### Probability of E: $( P(E) = \\frac{3}{6} = 0.5 )$\n",
    "\n",
    "### Drawing a Card from a Standard 52-Card Deck:\n",
    "#### Sample Space: 52 cards\n",
    "#### Event A: Drawing a heart, $( |A| = 13 )$\n",
    "#### Probability of A: $( P(A) = \\frac{13}{52} = 0.25 )$\n",
    "#### Event B: Drawing a king, $( |B| = 4 )$\n",
    "#### Probability of B: $( P(B) = \\frac{4}{52} = \\frac{1}{13} )$\n",
    "#### Probability of drawing a heart AND a king $(A and B): ( P(A \\cap B) = \\frac{1}{52} )$ (the king of hearts)\n",
    "#### Probability of drawing a heart OR a king $(A or B): ( P(A \\cup B) = P(A) + P(B) - P(A \\cap B) = \\frac{13}{52} + \\frac{4}{52} - \\frac{1}{52} = \\frac{16}{52} = \\frac{4}{13} )$\n",
    "\n",
    "### Independent Events: Flipping a coin twice. The outcome of the first flip does not affect the second.\n",
    "#### P(Heads on first flip) = 0.5\n",
    "#### P(Heads on second flip) = 0.5\n",
    "#### P(Heads on both flips) = $( 0.5 \\times 0.5 = 0.25 )$\n",
    "#### Conditional Probability: Drawing two cards without replacement.\n",
    "#### $P(Second card is a king | First card is a king) = ( \\frac{3}{51} )$ (since one king has been removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7e749-6ed0-413e-9ba6-bebda73d05d4",
   "metadata": {},
   "source": [
    "# ðŸ”¸II. Probability Distributions\n",
    "## Definition\n",
    "### A probability distribution is a function that describes the likelihood of obtaining the possible values that a random variable can assume. It can be discrete or continuous.\n",
    "### Random Variable: A variable whose value is a numerical outcome of a random phenomenon.\n",
    "### Discrete Random Variable: A variable that can take on a finite or countably infinite number of distinct values (e.g., number of heads in coin flips). Its distribution is often described by a Probability Mass Function (PMF).\n",
    "### Continuous Random Variable: A variable that can take on any value within a given range (e.g., height, temperature). Its distribution is often described by a Probability Density Function (PDF).\n",
    "\n",
    "## Common Probability Distributions\n",
    "### Discrete Distributions:\n",
    "#### Bernoulli Distribution: Models a single trial with two possible outcomes (success or failure). Parameter: ( p ) (probability of success). $( P(X=k) = p^k (1-p)^{1-k} ) for ( k \\in {0, 1} )$.\n",
    "#### Binomial Distribution: Models the number of successes in a fixed number of independent Bernoulli trials. Parameters: ( n ) (number of trials), ( p ) (probability of success). $( P(X=k) = \\binom{n}{k} p^k (1-p)^{n-k} ) for ( k = 0, 1, ..., n )$.\n",
    "#### Poisson Distribution: Models the number of events occurring in a fixed interval of time or space if these events occur with a known average rate and independently of the time since the last event. Parameter: $( \\lambda ) (average rate). ( P(X=k) = \\frac{e^{-\\lambda} \\lambda^k}{k!} ) for ( k = 0, 1, 2, ... )$.\n",
    "### Continuous Distributions:\n",
    "#### Uniform Distribution: All values within a given range are equally likely. Parameters: ( a ) (lower bound), ( b ) (upper bound). The PDF is $( f(x) = \\frac{1}{b-a} )$ for $( a \\le x \\le b )$, and 0 otherwise.\n",
    "#### Normal (Gaussian) Distribution: A symmetric, bell-shaped distribution characterized by its mean $(( \\mu ))$ and standard deviation $(( \\sigma ))$. Its PDF is $( f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2} )$. It is extremely important in statistics and machine learning.\n",
    "\n",
    "## Why Probability Distributions Matter in Machine Learning\n",
    "### Modeling Uncertainty: Machine learning deals with uncertain data and predictions. Probability distributions help quantify this uncertainty.\n",
    "### Statistical Inference: Many machine learning algorithms rely on statistical inference, which uses probability distributions.\n",
    "### Generative Models: Some models (e.g., Gaussian Mixture Models, Variational Autoencoders) explicitly model the probability distribution of the data.\n",
    "### Loss Functions: Concepts from probability (like maximum likelihood estimation) are used to define loss functions for training models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97273a27-bcfe-4d57-b339-1ddcc75db405",
   "metadata": {},
   "source": [
    "# ðŸ”¸III. Combinations\n",
    "## Definition\n",
    "### A combination is a selection of items from a set where the order of selection does not matter. The number of combinations of choosing ( k ) items from a set of ( n ) distinct items is given by the binomial coefficient:\n",
    "### $[ \\binom{n}{k} = \\frac{n!}{k!(n-k)!} ]$\n",
    "### where ( n! ) (n factorial) is the product of all positive integers up to ( n ).\n",
    "## Key Concepts\n",
    "### Order Doesn't Matter: Choosing apples then bananas is the same combination as choosing bananas then apples.\n",
    "### Distinct Items: Combinations typically deal with selecting from a set of unique items.\n",
    "## Examples\n",
    "### Choosing a Committee: How many ways can you choose a committee of 3 people from a group of 5?\n",
    "### ( n = 5 ) (total number of people)\n",
    "### ( k = 3 ) (number of people to choose)\n",
    "### Number of combinations = $( \\binom{5}{3} = \\frac{5!}{3!(5-3)!} = \\frac{5!}{3!2!} = \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{(3 \\times 2 \\times 1)(2 \\times 1)} = \\frac{120}{6 \\times 2} = \\frac{120}{12} = 10 )$ ways.\n",
    "## Selecting Cards: How many different 2-card hands can be dealt from a standard 52-card deck?\n",
    "### $( n = 52 )$\n",
    "### $( k = 2 )$\n",
    "### Number of combinations = $( \\binom{52}{2} = \\frac{52!}{2!(52-2)!} = \\frac{52!}{2!50!} = \\frac{52 \\times 51}{2 \\times 1} = 26 \\times 51 = 1326 )$ different hands.\n",
    "## Relationship to Probability\n",
    "### Combinations are crucial for calculating probabilities when the sample space involves selecting items and the order doesn't matter (as seen in some of the probability examples above, especially those involving cards or selections). The total number of possible outcomes in the sample space is often calculated using combinations.\n",
    "## Why Combinations Matter in Machine Learning\n",
    "### Feature Selection: In some feature selection techniques, we might need to consider different combinations of features.\n",
    "### Model Evaluation: When splitting data into training and testing sets, the number of possible splits can be calculated using combinations.\n",
    "### Hyperparameter Tuning: Exploring different combinations of hyperparameters for a model can involve combinatorial calculations.\n",
    "### Understanding Data Space: Combinatorial thinking can help in understanding the size and complexity of the possible data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649eb71d-64e0-4228-9511-1aa0805a585d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
